# Developer guide

## Introduction

GenomeDepot is based on the Django framework. Django architecture follows the Model-View-Template pattern, where Model handles the data representation, Template handles the presentation logic and View handles the business logic that takes up information from the user and displays model data to the user. GenomeDepot communicates with the web server over the Web Server Gateway Interface (WSGI), so Django views in GenomeDepot are synchronous in nature. However, there are several pages in GenomeDepot that employ asynchronous Javascript: BLAST search, database search in genes and annotations, comparative analyses, data export. In addition, long-running tasks started from the GenomeDepot administration panel, such as genome import or re-building BLAST databases, are executed asynchronously by the Django Q worker. Data models are defined in genomebrowser/browser/models.py.

## Where GenomeDepot stores sequence data

In GenomeDepot, only protein sequences are stored in the MySQL database. Nucleotide sequences are always stored in static files. STATIC_ROOT variable in the .env file defines the directory for static files. This directory has the “genomes” subdirectory for genome files. The genomes/gbff directory contains genomes in the Genbank format and the genomes/jbrowse directory contains files served by the embedded genome viewer (indexed FASTA file, indexed GFF3 files etc.) for each genome. The web server must have access to static files. BLAST databases are stored in the appdata subdirectory of the project directory. 

## Genome import pipeline

GenomeDepot imports genomes in batches. Since each run of the genome import pipeline re-generates BLAST databases and runs eggNOG-mapper, single genome import is impractical. A reasonable size of a batch for import is between 50 and 1000 genomes. 

The genome import pipeline runs several external tools generating ortholog family mappings, predicted operons and gene functional assignments. The first of them is eggNOG-mapper for fast functional annotation and orthology predictions. It processes all proteins from the imported genomes in chunks containing 200,000 sequences. GenomeDepot relies on eggNOG-mapper orthology predictions in comparative analyses and functional classifications. After the import of eggNOG-mapper output into database, the pipeline runs POEM for operon predictions, for one genome at a time. After importing operon predictions, the pipeline generates static files for the embedded Jbrowse browser and BLAST databases. At this moment, imported genomes become visible to web portal visitors. And finally, the genome import pipeline starts annotation pipeline, which runs an array of annotation tools.

## Annotation pipeline

The GenomeDepot annotation pipeline is an extensible toolkit of specialized annotation tools. Each tool in the pipeline can be configured and turned on or off in the administration panel. The annotation pipeline can be started as a part of the genome import process or independently for genomes that have been imported into GenomeDepot. The annotation pipeline runs gene annotation tools one by one for selected genomes, one genome at a time. 

## Architecture of annotation pipeline plugins

The annotation pipeline of GenomeDepot can be expanded with additional annotation tools. The annotation tools accept nucleotide or protein sequences as an input and generate functional annotations for individual genes. GenomeDepot interacts with annotation tools through plug-in Python modules that organize input files, execute external tools, process tool-specific outputs and generate tab-separated files imported by the GenomeDepot pipeline into the database.

A plug-in module must contain the application function that accepts two arguments. The first argument is an object of the Annotator class, and the second is a dictionary of genomes, with genome name as a key and a path to the GenBank file as a value. The application function returns full path of tab-separated text file with gene annotations.

Typically, a plug-in module implements three functions: preprocess, run and postprocess. The preprocess function creates a working directory in the GenomeDepot temporary directory and writes all input files into the working directory. The preprocess function also generates bash script that activates a Conda environment for the annotation tool, executes the tool and deactivates the Conda environment. The run function executes the bash script created by preprocess. The postprocess function reads output file(s) generated by the annotation tool and creates an output file for import into GenomeDepot in the temporary directory. Finally, the postprocess function deletes the working directory. 

## Plug-in configuration

Annotation tool plug-ins can be enabled or disabled in the GenomeDepot [administration portal](admin.md). To enable a plug-in, find the **“plugins.<tool_name>.enable”** parameter in the Configuration page (admin/browser/config/) and set it to 1. To disable a plug-in, set it to 0.

Other common plug-in configuration parameters are:

* **display_name:** the plug-in name in the administration portal pages

* **conda_env:** the name of Conda environment where the tool is installed (usually, genomedepot-<tool_name>)

* **threads:** number of threads for execution of the tool

Additional configuration parameters may store location of reference data files for the tool, default threshold values etc.

A plug-in module can access configuration parameters in the annotator.config dictionary of the annotator object passed as the first arguments to the application function.

## Annotation tool installation script

Genome annotation tools for the GenomeDepot annotation pipeline have to be installed in separate conda environment.

### Reference datafiles for annotation tools

## Writing input files with nucleotide and protein sequences for annotation tools

Many annotation tools accept input sequence files in one of standard file formats. GenomeDepot has utility functions for export genome and protein sequences in FASTA and GenBank formats in the browser.pipeline.util module:

* **export_proteins**: exports proteins from one or more genomes into a single FASTA file. Arguments: list of protein identifiers, output file name

* **export_proteins_bygenome**: exports proteins from one or more genomes into FASTA files, one file per genome. Arguments: dictionary of genome names and GenBank file paths, output directory

* **export_nucl_bygenome**: exports genome sequences into FASTA files. Arguments: dictionary of genome names and GenBank file paths, output directory

Example:
```
from pathlib import Path
from browser.models import Genome
from browser.pipeline.util import export_proteins, export_nucl_bygenome

# Exports all proteins from all genomes into the proteins.faa file
output_fasta_file = 'proteins.faa'
genome_ids = Genome.objects.values_list('id', flat=True)
export_proteins(genome_ids, output_fasta_file)

# Exports nucleotide sequences of all genomes into the /tmp/fna directory
output_dir = '/tmp/fna'
Path(output_dir).mkdir(exist_ok=True)
genomes = Genome.objects.values_list('name', 'gbk_filepath')
genome_data = {x[0]:x[1] for x in genomes}
export_nucl_bygenome(genome_data, output_dir)
```

## Temporary files
A temporary directory is stored in configuration parameter *core.temp_dir*. All temporary files should be kept in the temporary directory. It is highly recommended to create a subdirectory for each annotation tool.

Example:
```
from pathlib import Path

def application(genomes, annotator):
	preprocess(genomes, annotator)
	
def preprocess(genomes, annotator):
	tool_name = 'mytool'
	temp_dir = annotator.config['core.temp_dir']
	working_dir = os.path.join(temp_dir, tool_name)
	Path(working_dir).mkdir(parents=True,exist_ok=True)
```


[Back to start page](README.md)
